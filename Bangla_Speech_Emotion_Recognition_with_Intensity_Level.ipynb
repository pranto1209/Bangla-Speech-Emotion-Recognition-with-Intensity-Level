{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Ya-Q3Hg1b3",
        "outputId": "624c8639-5bea-4dc6-f943-447f1a4022ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV5_yG-tJHru",
        "outputId": "2cee3782-a56c-45c9-ab07-84b0c24d7e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-1\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/1-1-1-050.wav']\n",
            "(100, 196992)\n",
            "2-1\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-1-1-050.wav']\n",
            "(100, 196992)\n",
            "2-2\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/2-2-1-050.wav']\n",
            "(100, 196992)\n",
            "3-1\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-1-1-050.wav']\n",
            "(100, 196992)\n",
            "3-2\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/3-2-1-050.wav']\n",
            "(100, 196992)\n",
            "4-1\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-1-1-050.wav']\n",
            "(100, 196992)\n",
            "4-2\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/4-2-1-050.wav']\n",
            "(100, 196992)\n",
            "5-1\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-1-1-050.wav']\n",
            "(100, 196992)\n",
            "5-2\n",
            "['/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-001.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-002.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-003.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-004.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-005.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-006.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-007.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-008.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-009.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-010.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-011.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-012.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-013.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-014.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-015.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-016.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-017.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-018.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-019.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-020.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-021.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-022.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-023.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-024.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-025.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-026.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-027.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-028.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-029.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-030.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-031.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-032.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-033.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-034.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-035.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-036.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-037.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-038.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-039.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-040.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-0-050.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-041.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-042.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-043.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-044.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-045.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-046.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-047.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-048.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-049.wav', '/content/drive/MyDrive/Thesis/Dataset/Bangla/All/5-2-1-050.wav']\n",
            "(100, 196992)\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79]\n",
            "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "from numpy.core.fromnumeric import shape\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import librosa\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from librosa import display\n",
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Signal Transformation\n",
        "def signal_transformation(path):\n",
        "    y, sr = librosa.load(path, sr = 42666, duration = 3)\n",
        "    file_length = np.size(y)\n",
        "    if file_length < 128000:\n",
        "        y = np.concatenate((y, np.zeros(128000-file_length)), axis=0)\n",
        "    else:\n",
        "        y=y[0:128000]\n",
        "    \n",
        "    mfcc = librosa.feature.mfcc(y = y, sr = sr, hop_length = 250, n_mfcc = 128)\n",
        "    stft = np.abs(librosa.stft(y = y, n_fft = 254, hop_length = 250))\n",
        "    chroma = librosa.feature.chroma_stft(S = stft, n_chroma = 128)\n",
        "\n",
        "    #print(shape(mfcc))\n",
        "    #print(shape(stft))\n",
        "    #print(shape(chroma))\n",
        "    \n",
        "    log_mel_spectrogram = np.concatenate((mfcc,stft,chroma), axis=1)\n",
        "    mfcc =[]\n",
        "    stft =[]\n",
        "    chroma=[]\n",
        "    chro_cqt=[]\n",
        "    cqt =[]\n",
        "    y=[]\n",
        "    \n",
        "    log_mel_spectrogram = log_mel_spectrogram.reshape((-1,))\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "def classify_files(path):\n",
        "    dataset_dict = {\n",
        "        'total': 0,\n",
        "        'file_dict': {\n",
        "            '1-1': {'represent': 0, 'count': 0, 'all_data': []},\n",
        "            '2-1': {'represent': 1, 'count': 0, 'all_data': []},\n",
        "            '2-2': {'represent': 2, 'count': 0, 'all_data': []},\n",
        "            '3-1': {'represent': 3, 'count': 0, 'all_data': []},\n",
        "            '3-2': {'represent': 4, 'count': 0, 'all_data': []},\n",
        "            '4-1': {'represent': 5, 'count': 0, 'all_data': []},\n",
        "            '4-2': {'represent': 6, 'count': 0, 'all_data': []},\n",
        "            '5-1': {'represent': 7, 'count': 0, 'all_data': []},\n",
        "            '5-2': {'represent': 8, 'count': 0, 'all_data': []}\n",
        "        }\n",
        "    } \n",
        "        \n",
        "    wav_path = pathlib.Path(path)\n",
        "    emotion_file_list = [str(file_name) for file_name in wav_path.glob('*.wav')]\n",
        "    emotion_file_list.sort()\n",
        "    #print(emotion_file_list)\n",
        "    p = len(str(wav_path))\n",
        "\n",
        "    emotion_label_list = dataset_dict['file_dict'].keys()\n",
        "    for emotion_label in emotion_label_list:\n",
        "        print(emotion_label)\n",
        "        # emotion_classify_file_list = [letter for letter in emotion_file_list if letter[p+1:p+4] == emotion_label and letter[p+5:p+6] =='0']\n",
        "        emotion_classify_file_list = [letter for letter in emotion_file_list if letter[p+1:p+4] == emotion_label]\n",
        "        ck = 0\n",
        "        new_list = []\n",
        "        #female: 0, 49\n",
        "        indf = 0\n",
        "        #male: 50, 99\n",
        "        indm = 50\n",
        "        for i in range(0, 10):\n",
        "            for j in range(0, 10):\n",
        "                if (ck == 0):\n",
        "                    #print(indf)\n",
        "                    new_list.append(emotion_classify_file_list[indf])\n",
        "                    indf = indf + 1\n",
        "                else:\n",
        "                    #print(indm)\n",
        "                    new_list.append(emotion_classify_file_list[indm])\n",
        "                    indm = indm + 1\n",
        "            ck ^= 1\n",
        "\n",
        "        emotion_classify_file_list = new_list\n",
        "        print(emotion_classify_file_list)\n",
        "        files_count = len(emotion_classify_file_list)\n",
        "\n",
        "        dataset_dict['file_dict'][emotion_label]['count'] = files_count\n",
        "        dataset_dict['total'] = dataset_dict['total'] + files_count\n",
        "        emotion_data = [signal_transformation(path) for path in emotion_classify_file_list] \n",
        "        print(shape(emotion_data))\n",
        "        dataset_dict['file_dict'][emotion_label]['all_data'] = emotion_data \n",
        "        log_mel_spectrogram =[]\n",
        "        emotion_data = []\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "def load_data(path):\n",
        "    train_data_x = []\n",
        "    train_data_y = []\n",
        "    test_data_x = []\n",
        "    test_data_y = []\n",
        "\n",
        "    dataset_dict = classify_files(path)\n",
        "\n",
        "    emotion_label_list = dataset_dict['file_dict'].keys()\n",
        "    fold=5\n",
        "    temp=0\n",
        "    for emotion_label in emotion_label_list:\n",
        "        x = dataset_dict['file_dict'][emotion_label]['all_data']\n",
        "        count = dataset_dict['file_dict'][emotion_label]['count']\n",
        "        y = np.full(count, dataset_dict['file_dict'][emotion_label]['represent'])\n",
        "        \n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "        kfold = KFold(n_splits = 5, random_state = None, shuffle = False)\n",
        "        foldCount = 1\n",
        "        for train, test in kfold .split(x, y):\n",
        "            if (foldCount == fold):\n",
        "                print(train)\n",
        "                print(test)\n",
        "                print(fold)\n",
        "                train_data_x = np.append(train_data_x, x[train])\n",
        "                train_data_y = np.append(train_data_y, y[train])\n",
        "                test_data_x = np.append(test_data_x, x[test])\n",
        "                test_data_y = np.append(test_data_y, y[test])\n",
        "                x_train=[] \n",
        "                x_test=[]\n",
        "                y_train=[]\n",
        "                y_test =[]\n",
        "                x=[]\n",
        "                y=[]\n",
        "                break\n",
        "            else:\n",
        "                foldCount = foldCount + 1\n",
        "     \n",
        "   \n",
        "    dataset_dict=[]\n",
        "   \n",
        "    np.savetxt(path +'/data/train_data_y.csv', train_data_y, delimiter=',')\n",
        "    np.savetxt(path +'/data/test_data_y.csv', test_data_y, delimiter=',')\n",
        "    \n",
        "    x = 128\n",
        "    y = 513\n",
        "    z = 3\n",
        "    train_data_x = np.array(train_data_x).reshape(-1, x, y,z,1)\n",
        "    train_data_y = np.array(train_data_y)\n",
        "    test_data_x = np.array(test_data_x).reshape(-1, x, y,z,1)\n",
        "    test_data_y = np.array(test_data_y)\n",
        "\n",
        "    return train_data_x,train_data_y,test_data_x,test_data_y \n",
        "\n",
        "\n",
        "path='/content/drive/MyDrive/Thesis/Dataset/Bangla/All'\n",
        "\n",
        "train_data_x,train_data_y,test_data_x,test_data_y =load_data(path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2x6rSiEXZvJ"
      },
      "source": [
        "# Model Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUSJyQXUQBR7"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import shape\n",
        "import librosa\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pywt\n",
        "import csv\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import normalize, to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras,nn\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "###################################################################################################\n",
        "def model3d(input_shape, num_classes):\n",
        "    model = keras.Sequential(name='model3d')\n",
        "    # FB1\n",
        "    model.add(layers.Conv3D(filters=64, kernel_size=5, strides=1, padding='same', input_shape = input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1))) \n",
        "    # FB2\n",
        "    model.add(layers.Conv3D(filters=64, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n",
        "    # FB3\n",
        "    model.add(layers.Conv3D(filters=128, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1))) \n",
        "    # FB4\n",
        "    model.add(layers.Conv3D(filters=128, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling3D(pool_size=(4,4,1), strides=(4,4,1)))\n",
        "\n",
        "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    model.add(LSTM(256))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "############################################################################################################\n",
        "def model2d(input_shape, num_classes):\n",
        "    model = keras.Sequential(name='model2d')\n",
        "    #LFLB1\n",
        "    model.add(layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "    #LFLB2\n",
        "    model.add(layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    #LFLB3\n",
        "    model.add(layers.Conv2D(filters=128, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    #LFLB4\n",
        "    model.add(layers.Conv2D(filters=128, kernel_size=5, strides=1, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "\n",
        "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    model.add(LSTM(256))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def model7CNNFC(input_shape, num_classes):\n",
        "    model = keras.Sequential(name='model7cnnfc')\n",
        "\n",
        "    # FB1\n",
        "    model.add(layers.Conv2D(filters=16,kernel_size=(7,7),strides=2,padding='same',input_shape=input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB2\n",
        "    model.add(layers.Conv2D(filters=32,kernel_size=(5,5),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB3\n",
        "    model.add(layers.Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB4\n",
        "    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB5\n",
        "    model.add(layers.Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB6\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    # FB7\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation=nn.relu))\n",
        "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
        "\n",
        "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def model4CNNLSTM(input_shape, num_classes):\n",
        "    model = keras.Sequential(name='model4cnnlstm')\n",
        "    # FB1\n",
        "    model.add(layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "    # FB2\n",
        "    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    # FB3\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    # FB4\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('elu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "\n",
        "    model.add(layers.Reshape((-1, 256)))\n",
        "\n",
        "    #LSTM\n",
        "    model.add(layers.LSTM(256))\n",
        "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
        "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def model4CNNTDFBLSTM(input_shape, num_classes):\n",
        "    model = keras.Sequential(name='model2d')\n",
        "    # FB1\n",
        "    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "    # FB2\n",
        "    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    # FB3\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "    # FB4\n",
        "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
        "\n",
        "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(512)))\n",
        "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
        "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(train_data_x, train_data_y,test_data_x, test_data_y,emotion,emotionNumber):\n",
        "    model = model3d(input_shape=(128, 513, 3, 1), num_classes=emotionNumber)\n",
        "    # model = model2d(input_shape=(128, 513, 3), num_classes=emotionNumber)\n",
        "    # model = model7CNNFC(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    # model = model4CNNLSTM(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    # model = model4CNNTDFBLSTM(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=0, patience=20)\n",
        "    mc = ModelCheckpoint(path+'/model/'+emotion+'_max_model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n",
        "    history=model.fit(train_data_x, train_data_y, validation_data=(test_data_x, test_data_y), epochs=100, batch_size=16, verbose=0, callbacks=[es,mc])\n",
        "\n",
        "    # los = history.history['loss']\n",
        "    # vlos = history.history['val_loss']\n",
        "    # lvl = 'Loss_vs_Epochs_Proposed Model_' + emotion\n",
        "    # datas = {'Train_Loss':los, 'Test_Loss':vlos}\n",
        "    # lsvls = pd.DataFrame(datas)\n",
        "    # lsvls.to_csv(f'/content/drive/MyDrive/Thesis/CSV/{lvl}.csv', index = False)\n",
        "\n",
        "    # acc = history.history['categorical_accuracy']\n",
        "    # vacc = history.history['val_categorical_accuracy']\n",
        "    # ava = 'Accuracy_vs_Epochs_Proposed Model_' + emotion\n",
        "    # datas = {'Train_Accuracy':acc, 'Test_Accuracy':vacc}\n",
        "    # lsvls = pd.DataFrame(datas)\n",
        "    # lsvls.to_csv(f'/content/drive/MyDrive/Thesis/CSV/{ava}.csv', index = False)\n",
        "\n",
        "\n",
        "    model=[]\n",
        "    acc=history.history['categorical_accuracy']\n",
        "    vacc=history.history['val_categorical_accuracy']\n",
        "    loss=history.history['loss']\n",
        "    vloss=history.history['val_loss']\n",
        "    accLoss=[]\n",
        "    accLoss.append(acc)\n",
        "    accLoss.append(vacc)\n",
        "    accLoss.append(loss)\n",
        "    accLoss.append(vloss)\n",
        "    acc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n",
        "    accLoss=[]\n",
        "    model=[]\n",
        "    # model.save(path+'/model/'+emotion+'_model.h5')\n",
        "    return acc\n",
        "\n",
        "def test(test_data_x, test_data_y,emotion):\n",
        "    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n",
        "    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n",
        "    predict=new_model.predict(test_data_x)\n",
        "    return history[1]\n",
        "\n",
        "def maxIndex(data):\n",
        "    max=data[0]\n",
        "    index=0\n",
        "    for i in range(1,len(data)):\n",
        "        if(max<data[i]):\n",
        "            max=data[i]\n",
        "            index=i\n",
        "    return index  \n",
        "\n",
        "def test_emotion(test_data_x, test_data_y,total,emotion):\n",
        "    new_model = load_model(path+'/model/'+emotion+'_max_model.h5')\n",
        "    predict=new_model.predict(test_data_x)\n",
        "    test_data_y=np.argmax(test_data_y, axis=1)\n",
        "    temp=total+2\n",
        "    count=[[0]*temp]*temp\n",
        "    count=np.array(count)\n",
        "    #print(count)\n",
        "    for i in range(0,len(test_data_y)):\n",
        "        predictNew=maxIndex(predict[i])\n",
        "        #print(str(test_data_y[i])+' -- '+ str(predictNew))\n",
        "        count[test_data_y[i]][predictNew]=count[test_data_y[i]][predictNew]+1\n",
        "    for i in range(0,total):\n",
        "        for j in range(0,total):\n",
        "            if(i==j):\n",
        "                count[i][total]=count[i][j]\n",
        "                count[i][total+1]=sum(count[i][0:total])-count[i][j]\n",
        "\n",
        "    with open(path +'/data/'+emotion+'_confution.csv',\"w+\") as my_csv:\n",
        "        csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "        csvWriter.writerows(count)\n",
        "    return count"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SENkCgvhb6e_"
      },
      "source": [
        "#5 Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGaGu6d7Xjnw",
        "outputId": "dc0e79ef-ebea-45f5-90ba-490d6fa22936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "720\n",
            "180\n",
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128, 513, 3, 64)  256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 64, 256, 3, 64)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64, 256, 3, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 16, 64, 3, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 64, 3, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 4, 16, 3, 128)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4, 16, 3, 128)    512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 1, 4, 3, 128)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 1, 1536)          0         \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 1, 512)           3672064   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,054,725\n",
            "Trainable params: 8,053,957\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "-----------------------------------------------------------------\n",
            "[13, 1, 1, 3, 2, 13, 7, 65.0]\n",
            "[5, 28, 1, 2, 4, 28, 12, 70.0]\n",
            "[2, 2, 32, 1, 3, 32, 8, 80.0]\n",
            "[0, 0, 0, 35, 5, 35, 5, 87.5]\n",
            "[1, 0, 3, 8, 28, 28, 12, 70.0]\n",
            "Accuracy: 75.56\n"
          ]
        }
      ],
      "source": [
        "def lebelTo5(data):\n",
        "    print(len(data))\n",
        "    for i in range(len(data)):\n",
        "        if(int(data[i])%2!=0):\n",
        "            data[i]=int(data[i])+1\n",
        "        data[i]=data[i]/2\n",
        "    return data\n",
        "train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n",
        "#validation_data_y = np.array(np.genfromtxt(path +'/data/validation_data_y.csv', delimiter=','))\n",
        "test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n",
        "train_data_y = to_categorical(lebelTo5(train_data_y))\n",
        "#validation_data_y = to_categorical(lebelTo5(validation_data_y))\n",
        "test_data_y = to_categorical(lebelTo5(test_data_y))\n",
        "\n",
        "emotion='5_emotion'\n",
        "#crossValidationTrain(train_data_x, train_data_y,5)\n",
        "\n",
        "acc=train(train_data_x, train_data_y,test_data_x, test_data_y,emotion,5)\n",
        "result = test_emotion(test_data_x, test_data_y,5,emotion)\n",
        "result = np.array(result)\n",
        "\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "accuracy=[]; \n",
        "for i in range(0,5):\n",
        "    accuracy.append([0]*8);\n",
        "total_data=0\n",
        "total_correct=0\n",
        "for i in range(0,5):\n",
        "    for j in range(0,8):\n",
        "        if(j!=7):\n",
        "            accuracy[i][j]=result[i][j]\n",
        "        else:\n",
        "            total=result[i][j-1]+result[i][j-2]\n",
        "            total_data+=total\n",
        "            total_correct+=result[i][j-2]\n",
        "            accuracy[i][j]=round(result[i][j-2]*100/total, 2)\n",
        "            \n",
        "for i in range(0,len(accuracy)):\n",
        "    print(accuracy[i])\n",
        "\n",
        "print(\"Accuracy: \", end = \"\")\n",
        "print(round(total_correct*100/total_data, 2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RNlYv_C3sC0h"
      },
      "source": [
        "#4 Emotion 2 Grade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYCklbtqPksc",
        "outputId": "c3c4e459-4ca6-4f24-b369-bef9d4c115c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180\n",
            "6/6 [==============================] - 4s 410ms/step\n",
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_24 (Conv3D)          (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 128, 513, 3, 64)  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d_24 (MaxPoolin  (None, 64, 256, 3, 64)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_25 (Conv3D)          (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 64, 256, 3, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_25 (MaxPoolin  (None, 16, 64, 3, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_26 (Conv3D)          (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 16, 64, 3, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_26 (MaxPoolin  (None, 4, 16, 3, 128)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_27 (Conv3D)          (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 4, 16, 3, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_27 (MaxPoolin  (None, 1, 4, 3, 128)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 1, 1536)          0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 1, 512)           3672064   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 256)               787456    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,053,954\n",
            "Trainable params: 8,053,186\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "180\n",
            "6/6 [==============================] - 4s 412ms/step\n",
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_28 (Conv3D)          (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 128, 513, 3, 64)  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d_28 (MaxPoolin  (None, 64, 256, 3, 64)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_29 (Conv3D)          (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 64, 256, 3, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_29 (MaxPoolin  (None, 16, 64, 3, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_30 (Conv3D)          (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 16, 64, 3, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_30 (MaxPoolin  (None, 4, 16, 3, 128)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_31 (Conv3D)          (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 4, 16, 3, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_31 (MaxPoolin  (None, 1, 4, 3, 128)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, 1, 1536)          0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 1, 512)           3672064   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 256)               787456    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,053,954\n",
            "Trainable params: 8,053,186\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "2/2 [==============================] - 2s 13ms/step\n",
            "180\n",
            "6/6 [==============================] - 4s 414ms/step\n",
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_32 (Conv3D)          (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 128, 513, 3, 64)  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_32 (Activation)  (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d_32 (MaxPoolin  (None, 64, 256, 3, 64)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_33 (Conv3D)          (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 64, 256, 3, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_33 (Activation)  (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_33 (MaxPoolin  (None, 16, 64, 3, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_34 (Conv3D)          (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16, 64, 3, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_34 (Activation)  (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_34 (MaxPoolin  (None, 4, 16, 3, 128)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_35 (Conv3D)          (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 4, 16, 3, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_35 (Activation)  (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_35 (MaxPoolin  (None, 1, 4, 3, 128)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 1, 1536)          0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 1, 512)           3672064   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 256)               787456    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,053,954\n",
            "Trainable params: 8,053,186\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "180\n",
            "6/6 [==============================] - 4s 414ms/step\n",
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_36 (Conv3D)          (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 128, 513, 3, 64)  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d_36 (MaxPoolin  (None, 64, 256, 3, 64)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_37 (Conv3D)          (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 64, 256, 3, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_37 (MaxPoolin  (None, 16, 64, 3, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_38 (Conv3D)          (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 16, 64, 3, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_38 (MaxPoolin  (None, 4, 16, 3, 128)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_39 (Conv3D)          (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 4, 16, 3, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_39 (Activation)  (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_39 (MaxPoolin  (None, 1, 4, 3, 128)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 1, 1536)          0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 1, 512)           3672064   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 256)               787456    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,053,954\n",
            "Trainable params: 8,053,186\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Happy\n",
            "[14, 1, 14, 1, 93.33]\n",
            "[0, 13, 13, 0, 100.0]\n",
            "Accuracy: 96.43\n",
            "----------------------------------------------------------------\n",
            "Sad\n",
            "[15, 0, 15, 0, 100.0]\n",
            "[1, 16, 16, 1, 94.12]\n",
            "Accuracy: 96.88\n",
            "----------------------------------------------------------------\n",
            "Angry\n",
            "[15, 2, 15, 2, 88.24]\n",
            "[0, 18, 18, 0, 100.0]\n",
            "Accuracy: 94.29\n",
            "----------------------------------------------------------------\n",
            "Disgust\n",
            "[11, 2, 11, 2, 84.62]\n",
            "[1, 14, 14, 1, 93.33]\n",
            "Accuracy: 89.29\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 2 emotion or intensity\n",
        "def train_2(train_data_x, train_data_y,test_data_x, test_data_y,emotion,emotionNumber):\n",
        "    model = model3d(input_shape=(128, 513, 3, 1), num_classes=emotionNumber)\n",
        "    # model = model2d(input_shape=(128, 513, 3), num_classes=emotionNumber)\n",
        "    # model = model7CNNFC(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    # model = model4CNNLSTM(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    # model = model4CNNTDFBLSTM(input_shape=(128, 513, 1), num_classes=emotionNumber)\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_categorical_accuracy',mode='max',verbose=0,patience=20)\n",
        "    mc = ModelCheckpoint(path+'/model/'+emotion+'_max_model.h5',monitor='val_categorical_accuracy',mode='max',verbose=0,save_best_only=True)\n",
        "    history=model.fit(train_data_x, train_data_y, validation_data=(test_data_x, test_data_y), epochs=100, batch_size=16,verbose=0, callbacks=[es,mc])\n",
        "    \n",
        "    acc=history.history['categorical_accuracy']\n",
        "    vacc=history.history['val_categorical_accuracy']\n",
        "    loss=history.history['loss']\n",
        "    vloss=history.history['val_loss']\n",
        "    accLoss=[]\n",
        "    accLoss.append(acc)\n",
        "    accLoss.append(vacc)\n",
        "    accLoss.append(loss)\n",
        "    accLoss.append(vloss)\n",
        "\n",
        "    acc=history.history['categorical_accuracy'][len(history.history['categorical_accuracy']) - 1]\n",
        "    accLoss=[]\n",
        "    model=[]\n",
        "    #model.save(path+'/model/'+emotion+'_model.h5')\n",
        "    return acc\n",
        "\n",
        "\n",
        "def predictTest(test_data_x, test_data_y_5,test_data_y_3,sl):\n",
        "    new_model = load_model(path+'/model/5_emotion_max_model.h5')\n",
        "    predict=new_model.predict((test_data_x))\n",
        "    \n",
        "    tempTestX=[]\n",
        "    tempTestY=[]\n",
        "    count=0\n",
        "    for i in range(len(test_data_y_5)):\n",
        "        predict_5=maxIndex(predict[i])\n",
        "        if(predict_5==sl and predict_5==test_data_y_5[i]):\n",
        "            tempTestX.append(test_data_x[i])\n",
        "            tempTestY.append(test_data_y_3[i])\n",
        "            count=count+1\n",
        "    tempTestX=np.array(tempTestX)\n",
        "    tempTestY=np.array(tempTestY)\n",
        "\n",
        "    return tempTestX, tempTestY\n",
        "\n",
        "\n",
        "def labelTo2All(train_data_y,train_data_x,label):\n",
        "    label=label*2\n",
        "    tempX=[]\n",
        "    tempY=[]\n",
        "    for i in range(len(train_data_y)):\n",
        "        if(int(train_data_y[i])+1==label):\n",
        "            tempY.append(0)\n",
        "            tempX.append(train_data_x[i])\n",
        "        elif(int(train_data_y[i])== label):\n",
        "            tempY.append(1)\n",
        "            tempX.append(train_data_x[i])\n",
        "\n",
        "    tempX=np.array(tempX)\n",
        "    tempY=np.array(tempY)\n",
        "    return tempX,tempY\n",
        "\n",
        "\n",
        "def lebelTo5(data):\n",
        "    print(len(data))\n",
        "    for i in range(len(data)):\n",
        "        if(int(data[i])%2!=0):\n",
        "            data[i]=int(data[i])+1\n",
        "        data[i]=data[i]/2\n",
        "    return data\n",
        "\n",
        "\n",
        "def lebelTo3(train_data_y):\n",
        "    tempX=[]\n",
        "    tempY=[]\n",
        "    for i in range(len(train_data_y)):\n",
        "        if(int(train_data_y[i])%2==0 and int(train_data_y[i])!=0):\n",
        "            train_data_y[i]=1\n",
        "        elif(int(train_data_y[i])!=0):\n",
        "            train_data_y[i]=0\n",
        "        else:\n",
        "            train_data_y[i]=2\n",
        "    return train_data_y\n",
        "\n",
        "\n",
        "emotion = ['neutral','happy','sad','angry','disgust']\n",
        "all = []\n",
        "for i in range(1,len(emotion)):\n",
        "    train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n",
        "    tempTrainX,tempTrainY = labelTo2All(train_data_y,train_data_x,i)\n",
        "\n",
        "    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n",
        "    test_data_y_5 = lebelTo5(test_data_y)\n",
        "\n",
        "    test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n",
        "    test_data_y_3 = lebelTo3(test_data_y)\n",
        "    \n",
        "    tempTestX, tempTestY = predictTest(test_data_x, test_data_y_5,test_data_y_3,i)\n",
        "    acc = train_2(tempTrainX, to_categorical(tempTrainY),tempTestX,to_categorical(tempTestY),emotion[i],2)\n",
        "    res = test_emotion(tempTestX, to_categorical(tempTestY),2,emotion[i])\n",
        "    res = np.array(res)\n",
        "    all.append(res)\n",
        "\n",
        "id = 0\n",
        "for result in all:\n",
        "    id += 1\n",
        "    if id == 1:\n",
        "        print(\"Happy\")\n",
        "    elif id == 2:\n",
        "        print(\"Sad\")\n",
        "    elif id == 3:\n",
        "        print(\"Angry\")\n",
        "    elif id == 4:\n",
        "        print(\"Disgust\")\n",
        "    \n",
        "    accuracy=[]; \n",
        "    for i in range(0,2):\n",
        "        accuracy.append([0]*5);\n",
        "    total_data=0\n",
        "    total_correct=0\n",
        "    for i in range(0,2):\n",
        "        for j in range(0,5):\n",
        "            if(j!=4):\n",
        "                accuracy[i][j]=result[i][j]\n",
        "            else:\n",
        "                total=result[i][j-1]+result[i][j-2]\n",
        "                total_data+=total\n",
        "                total_correct+=result[i][j-2]\n",
        "                accuracy[i][j]=round(result[i][j-2]*100/total, 2)\n",
        "\n",
        "    for i in range(0,len(accuracy)):\n",
        "        print(accuracy[i])\n",
        "        \n",
        "    print(\"Accuracy: \", end = \"\")\n",
        "    print(round(total_correct*100/total_data, 2))\n",
        "    print(\"----------------------------------------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EQKrvzGtbny5"
      },
      "source": [
        "#9 Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7Glu9Q5mcdK",
        "outputId": "871b9bfa-3c64-4e74-9bf2-8d81a9f8ccc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model3d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_4 (Conv3D)           (None, 128, 513, 3, 64)   8064      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 128, 513, 3, 64)  256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 128, 513, 3, 64)   0         \n",
            "                                                                 \n",
            " max_pooling3d_4 (MaxPooling  (None, 64, 256, 3, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_5 (Conv3D)           (None, 64, 256, 3, 64)    512064    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 64, 256, 3, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 64, 256, 3, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_5 (MaxPooling  (None, 16, 64, 3, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_6 (Conv3D)           (None, 16, 64, 3, 128)    1024128   \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16, 64, 3, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16, 64, 3, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_6 (MaxPooling  (None, 4, 16, 3, 128)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 4, 16, 3, 128)     2048128   \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 16, 3, 128)    512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4, 16, 3, 128)     0         \n",
            "                                                                 \n",
            " max_pooling3d_7 (MaxPooling  (None, 1, 4, 3, 128)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 1, 1536)          0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 1, 512)           3672064   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 2313      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,055,753\n",
            "Trainable params: 8,054,985\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "-----------------------------------------------------------------\n",
            "[11, 3, 0, 2, 0, 1, 1, 1, 1, 11, 9, 55.0]\n",
            "[1, 10, 1, 2, 2, 2, 0, 2, 0, 10, 10, 50.0]\n",
            "[1, 5, 12, 0, 0, 0, 0, 2, 0, 12, 8, 60.0]\n",
            "[0, 2, 0, 11, 1, 1, 1, 2, 2, 11, 9, 55.0]\n",
            "[0, 2, 0, 1, 17, 0, 0, 0, 0, 17, 3, 85.0]\n",
            "[0, 0, 0, 0, 2, 13, 4, 1, 0, 13, 7, 65.0]\n",
            "[0, 0, 0, 1, 0, 5, 14, 0, 0, 14, 6, 70.0]\n",
            "[1, 1, 0, 0, 1, 5, 1, 10, 1, 10, 10, 50.0]\n",
            "[0, 0, 0, 0, 0, 0, 3, 5, 12, 12, 8, 60.0]\n",
            "Accuracy: 61.11\n"
          ]
        }
      ],
      "source": [
        "train_data_y = np.array(np.genfromtxt(path +'/data/train_data_y.csv', delimiter=','))\n",
        "#validation_data_y = np.array(np.genfromtxt(path +'/data/validation_data_y.csv', delimiter=','))\n",
        "test_data_y = np.array(np.genfromtxt(path +'/data/test_data_y.csv', delimiter=','))\n",
        "\n",
        "train_data_y = to_categorical(train_data_y)\n",
        "#validation_data_y = to_categorical(validation_data_y)\n",
        "test_data_y = to_categorical(test_data_y)\n",
        "emotion='9_emotion'\n",
        "#acc,vacc=train(train_data_x, train_data_y, validation_data_x, validation_data_y,emotion,9)\n",
        "acc=train(train_data_x, train_data_y,test_data_x, test_data_y,emotion,9)\n",
        "\n",
        "emotion = '9_emotion'\n",
        "result = test_emotion(test_data_x, test_data_y,9,emotion)\n",
        "result = np.array(result)\n",
        "\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "accuracy=[]; \n",
        "for i in range(0,9):\n",
        "    accuracy.append([0]*12);\n",
        "total_data=0\n",
        "total_correct=0\n",
        "for i in range(0,9):\n",
        "    for j in range(0,12):\n",
        "        if(j!=11):\n",
        "            accuracy[i][j]=result[i][j]\n",
        "        else:\n",
        "            total=result[i][j-1]+result[i][j-2]\n",
        "            total_data+=total\n",
        "            total_correct+=result[i][j-2]\n",
        "            accuracy[i][j]=round(result[i][j-2]*100/total, 2)\n",
        "            \n",
        "for i in range(0,len(accuracy)):\n",
        "    print(accuracy[i])\n",
        "\n",
        "print(\"Accuracy: \", end = \"\")\n",
        "print(round(total_correct*100/total_data, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
